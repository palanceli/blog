---
layout: post
title: DeepLearning.ai笔记（三）
date: 2018-03-17 10:00:00 +0800
categories: 机器学习
tags: 随笔
toc: true
comments: true
mathjax: true
---
本节的题目是深层神经网络，只是把网络的层数延展为变量。使用的方法还是前两节的方法——梯度下降、正向传播、反向传播。
<!-- more -->

# 4.1 深层神经网络
本节内容是在上一节双层神经网络的基础上将网络层数扩展为变量。讨论L层神经网络的正向和反向传播算法，因此也并没有引入什么新知识。  
在命名体系中引入了L表示神经网络的层数，$n^{[l]}$表示第l层节点的个数，$a^{[l]}$表示第l层的激活函数。  
在下图中：
![](0317DeepLearningAI04/img01.png)
L=4， $n^{[1]}=5, n^{[2]}=5, n^{[3]}=3, n{[4]}=1$

# 4.2 深层神经网络的前向传播
输入X：  
$\begin{cases}z^{[1]}=w^{[1]}·x + b^{[1]}=w^{[1]}·a^{[0]} + b^{[1]} \\ 
a^{[1]} = g^{[1]}(z^{[1]}) \end{cases}\\ 
\begin{cases}z^{[2]}=w^{[2]}·a^{[1]} + b^{[2]} \\ 
a^{[2]} = g^{[2]}(z^{[2]})\end{cases}\\ 
... \\
\begin{cases}z^{[L]}=w^{[L]}·a^{[L-1]} + b^{[L]} \\ 
a^{[L]} = g^{[L]}(z^{[L]})=ŷ\end{cases}$  

向量化后：  
$\begin{cases}Z^{[1]}=W^{[1]}·X + b^{[1]}=W^{[1]}·A^{[0]} + b^{[1]} \\ 
A^{[1]} = g^{[1]}(Z^{[1]}) \end{cases}\\ 
\begin{cases}Z^{[2]}=W^{[2]}·A^{[1]} + b^{[2]} \\ 
A^{[2]} = g^{[2]}(Z^{[2]})\end{cases}\\ 
... \\
\begin{cases}Z^{[L]}=W^{[L]}·A^{[L-1]} + b^{[L]} \\ 
A^{[L]} = g^{[L]}(Z^{[L]})=Ŷ\end{cases}$  

# 4.3 核对矩阵的维数
$X=\begin{bmatrix}x_{1}\\x_{2}\\\cdots\\x_{(n^{[0]})}\end{bmatrix}\in\mathbb{R}^{n^{[0]}×1}，\; W^{[1]}=\begin{bmatrix}—w_{1}^{[1]}— \cr —w_{2}^{[1]}— \cr ... \cr —w_{n^{[1]}}^{[1]}— \end{bmatrix}\in\mathbb{R}^{n^{[1]}×n^{[0]}}，\; b^{[1]}\in\mathbb{R}^{n^{[1]}×1}\\
于是W^{[1]}·X\in\mathbb{R}^{n^{[1]}×1} ，\; Z^{[1]}=W^{[1]}·X + b^{[1]}\in\mathbb{R}^{n^{[1]}×1} ，\; A^{[1]}\in\mathbb{R}^{n^{[1]}×1}$  
$W^{[2]}=\begin{bmatrix}—w_{1}^{[2]}— \cr —w_{2}^{[2]}— \cr ... \cr —w_{n^{[2]}}^{[2]}— \end{bmatrix}\in\mathbb{R}^{n^{[2]}×n^{[1]}}，\; b^{[2]}\in\mathbb{R}^{n^{[2]}×1}\\
于是W^{[2]}·A^{[1]}\in\mathbb{R}^{n^{[2]}×1} ，\; Z^{[2]}=W^{[2]}·A^{[1]} + b^{[2]}\in\mathbb{R}^{n^{[2]}×1} ，\; A^{[2]}\in\mathbb{R}^{n^{[2]}×1}$  
$同理W^{[l]}·A^{[l]}\in\mathbb{R}^{n^{[l]}×1} ，\; Z^{[l]}=W^{[l]}·A^{[l-1]} + b^{[l]}\in\mathbb{R}^{n^{[l]}×1} ，\; A^{[l]}\in\mathbb{R}^{n^{[l]}×1}$

# 4.4 为什么使用深层表示
本节以图像识别为例，神经网络中，浅层神经元反映的是图像的细部特征，比如棱角、边缘，越往深层，越反应图像的整体特征，例如无关、以及再往深了反映出相貌特征。  
所以直观地理解，如果识别“图像是关于人、狗、还是猫”，可能是用较浅层的神经网络即可，如果要识别具体是哪个人、哪条狗，则需要更深层网络。  
不过我觉得这个解释依然是骗直觉的。为什么层次越深越能抽象整体特征？以及究竟需要多深，本文并没有给出解释。

# 4.5 搭建深层神经网络块
根据本节4.2已经得出正向传播：  
$\begin{cases}Z^{[l]}=W^{[l]}·A^{[l-1]} + b^{[l]}  …①\\ 
A^{[l]} = g^{[l]}(Z^{[l]}) 　　　　…②\end{cases}$

再看反向传播，回顾在[逻辑回归的损失函数和成本函数](/2018/03/11/2018/0311DeepLearningAI02/#逻辑回归的损失函数和成本函数)中引入的损失函数：  
$L(ŷ, y)=-(y\logŷ + (1-y)\log(1-ŷ))$  
因此对于L层神经网络来说，损失函数是关于y和输出层的函数：  
$L(a, y)=-(y\log{a} + (1-y)\log{(1-a)})$  
其中a即$A^{[L]}$的结果，由[逻辑回归中的梯度下降法](http://localhost:4000/2018/03/11/2018/0311DeepLearningAI02/#逻辑回归中的梯度下降法)可知：  
$da^{[L]}=-\frac{y}{a^{[L]}} + \frac{(1-y)}{(1-a^{[L]}}$  
$dZ^{[L]}=\frac{dL}{da^{[L]}}·\frac{da^{[L]}}{dZ^{[L]}}=da^{[L]}·g^{[L]}\prime(Z^{[L]})　　后半部分是根据②得出$  
$dW^{[L]}=\frac{dL}{dZ^{[L]}}·\frac{dZ^{[L]}}{dW^{[L]}}=\frac{1}{m}dZ^{[L]}·A^{[L-1]}　　后半部分是根据①得出$
$db^{[L]}=\frac{1}{m}np.sum(dZ^{[L]}, axis=1, keepdims=True)$  
$dA^{[L-1]}$怎么求呢？它是$\frac{dL}{dA^{[L-1]}}$的缩写，可是dL是关于$A^{[L]}$的函数，看不出与$A^{[L]}$的关系，但是由①和②可得到$A^{[L]}和A^{[L-1]}$的关系，根据链式法则：  
$dA^{[L-1]}=\frac{dL}{dA^{[L]}}·\frac{dA^{[L]}}{dA^{[L-1]}}\\
=\frac{dL}{dA^{[L]}}·\frac{dA^{[L]}}{dZ^{[L]}}·\frac{dZ^{[L]}}{dA^{[L-1]}}　　根据②\\
=\frac{dL}{dZ^{[L]}}·\frac{dZ^{[L]}}{dA^{[L-1]}}　　合并前两项\\
=dZ^{[L]}·W^{[L]}　　根据①\\$  

反向传播总结如下：  
$\begin{cases}dZ^{[l]}=da^{[l]}·g^{[l]}\prime(Z^{[l]})\\ 
dW^{[l]}=\frac{1}{m}dZ^{[l]}·A^{[l-1]} \\
db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]}, axis=1, keepdims=True)\\
dA^{[l-1]}=dZ^{[l]}·W^{[l]}\end{cases}$  

正向传播算法就是构造Z和A的形式，反向传播算法就是根据Z、A计算dW和db的形式，神经网络的算法主体则是根据超参数循环计算W和b：
![](0317DeepLearningAI04/img02.png)

# 4.6 前向传播和反向传播
上一节已经把两向传播算法推导出来了。

# 4.7 参数VS超参数
在神经网络算法中，参数是指：$W^{[1]}、b^{[1]}、W^{[2]}、b^{[2]}……$  
超参数是指：学习率α、迭代次数#iterations、隐藏层数 #hidden layers、激活函数

# 4.8 这和大脑有什么关系
本节解释了神经网络中的神经元与大脑神经元的形似之处，但Andrew也提到这种形似被越来越少得提及，因为人脑的神经元是如何工作的，至今并未完全弄清楚。“神经网络”只是一个听起来很高大上的名字，其实与人脑并没有太大关系。

# 作业