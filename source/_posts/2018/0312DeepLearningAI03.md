---
layout: post
title: DeepLearning.ai笔记（二）
date: 2018-03-12 10:00:00 +0800
categories: 机器学习
tags: 随笔
toc: true
comments: true
mathjax: true
---
本节的题目是浅层神经网络，开始接触多层结构，此时才真正接触到了神经网络模型。其实运用到的理论知识和上一节的逻辑回归相比，几乎没有新东西。

为了便于描述，我把前一节运算出来的a称为一个神经元（这是我的直观理解，并无出处~），该神经元与输入数据直接关联，并直接输出为二分结果。真实世界人脑的思考过程通常没有这么简单，有很多与之相关的成语比如“深思熟虑”、“三思而行”、“深谋远虑”……都是指有深度、反复的思考。本节课程正是通过增加神经元的层次、以及每一层的个数来增强思考的深度和广度。
<!-- more -->

# 3.1 神经网络概览

神经网络通常是具备两层以上的结构，但在原理上，仅是上一节内容的应用，并没有增加新的理论知识。在符号表示上使用方括号上标$z^{[1]}、a^{[1]}$来表示第几层，这与上一节的$x^{(i)}$是不同的，后者表示第i个样本。

在概念上，中间层的每一个节点都是应用了上一节的算法计算出来的，对于单样本逻辑回归，模型如下：
![](0312DeepLearningAI03/img01.png)
由此根据正向算法那计算神经元和成本函数：
![](0312DeepLearningAI03/img02.png)
神经网络则在逻辑回归的基础上增加了中间的z/a层，以及每一层的多个节点：
![](0312DeepLearningAI03/img03.png)
其中每个节点对上一层节点均使用了逻辑回归的正向算法，最后通过梯度下降法计算成本函数$L(a^{[2]}, y)$：
![](0312DeepLearningAI03/img04.png)

# 3.2 神经网络表示
这是一个最简单的神经网络，它具有双层结构，这两层指的是隐藏层和输出层：
![](0312DeepLearningAI03/img05.png)
输入层通常不作为一层，因为$a^{[1]}$是通过输入层X与构造出的参数$W^{[1]}和b^{[1]}$叠加运算而成，而$a^{[2]}$是$a^{[1]}$与构造出的参数$W^{[2]}和b^{[2]}$叠加运算而成。W和b构成了模型的本质，它们只有两层，因此说这是一个双层的神经网络模型。
对于单样本，假设输入层共$n^{[0]}$个特征，隐藏层共$n^{[1]}$个节点，输出层共$n^{[2]}$个节点，显然$n^{[2]}$=1，则

$X= \left\lgroup \matrix{x_{1} \cr x_{2} \cr ... \cr x_{n_{[0]}} } \right \rgroup \\ z_{1}^{[1]} = w_{1}^{[1]}·X + b^{[1]} = [w_{1}, w_{2}, ..., w_{n^{[0]}}] · \left\lgroup \matrix{x_{1} \cr x_{2} \cr ... \cr x_{n^{[0]}} } \right \rgroup + b^{[1]} \in \mathbb{R} \\ 隐藏层共有n^{[1]}个节点，于是：z^{[1]}=\left\lgroup \matrix{z_{1}^{[1]} \cr z_{2}^{[1]} \cr ... \cr z_{n^{[1]}}^{[1]} } \right \rgroup \in \mathbb{R}^{n^{[1]}\,× 1} 　　\; w^{[1]}=\begin{bmatrix}—w_{1}—\\—w_{2}—\\\vdots\\—w_{n^{[1]}}—\end{bmatrix} \in \mathbb{R}^{n^{[1]}\,×n^{[0]}}$

# 3.3 计算神经网络的输出
计算神经网络的输出就是应用神经元的计算法则，逐个节点求解：

$z^{[1]}=\left\lgroup \matrix{z_{1}^{[1]} \cr z_{2}^{[1]} \cr ... \cr x_{n_{[1]}}^{[1]} } \right \rgroup=\left\lgroup \matrix{w_{1}^{[1]}·X + b_{1}^{[1]} \cr w_{2}^{[1]}·X + b_{2}^{[1]} \cr ... \cr w_{n^{[1]}}^{[1]}·X + b_{n^{[1]}}^{[1]} } \right \rgroup=\begin{bmatrix}—w_{1}^{[1]}— \cr —w_{2}^{[1]}— \cr ... \cr —w_{n^{[1]}}^{[1]}— \end{bmatrix}·X + \begin{bmatrix}b_{1}^{[1]} \cr b_{2}^{[1]} \cr ... \cr b_{n^{[1]}}^{[1]} \end{bmatrix}$

$a^{[1]} = \begin{bmatrix}a_{1}^{[1]} \cr a_{2}^{[1]} \cr ... \cr a_{n^{[1]}}^{[1]} \end{bmatrix}=σ(z^{[1]})$

同理总结如下：

$z^{[1]}=w^{[1]}·x + b^{[1]} \\ a^{[1]} = σ(z^{[1]}) \\ z^{[2]}=w^{[2]}·a^{[1]} + b^{[2]} \\ a^{[2]} = σ(z^{[2]})$

# 思考
神经网络一方面增加了神经元的层次，提高思考的深度，另一方面也在同一层次上布局了多个神经元，提高了思考的广度。这些神经元彼此连接便成了“神经网络”。再次申明，这都是我的直观理解，并没有理论依据。这么做增加了思考的抽象程度，但是也更加让思考过程变得越发不可理解。为什么经过这么多层的运算就比简单“神经元”有效？每一层的含义是什么？神经网络的深度、每一层的神经元个数与模型的效果是什么关系？我不知道！继续学习吧。