---
layout: post
title: DeepLearning开悟
date: 2018-03-09 10:00:00 +0800
categories: 机器学习
tags: 随笔
toc: true
comments: true
---
完成DeepLearning.AI的第一个编程作业后，突然觉得对深度学习开悟了。把之前的学习、感悟和作业代码记录下来。
<!-- more -->
前几个月的学习要么太偏向理论课程，要么又是如何操作API的所谓实战。前者我啃了好多遍的梯度下降法，总不知道这玩意儿跟图片识别、翻译、自然语言处理有什么关系；后者我虽然能运用TensorFlow做一些事情，但仅仅是知道如何操作API而已，那和机器学习的原理其实没啥关系。

做完DeepLearning.AI第一个编程作业后，终于能够把理论和实践结合在一起了，感觉就好像越过了一道山丘，眼前一亮看到梦寐已久的美丽天地。

# 我所理解的神经网络
人脑的工作基础是基于神经元，每个神经元有开/闭两种状态。人脑里有上亿个神经元，看到一只猫会触发大脑里若干个神经元通路闭合，这就把“猫”和这些神经元的状态建立了关联。对于一张图片，能不能找到这样一个等式：
`x1 * w1 + x2 * w2 + ... + xn * wn = P`
其中x1..xn是若干个神经元，w1..wn（记作W）是若干个参数，可以理解为每个神经元的权重，P是这张图是否有猫的概率。具体需要多少个神经元呢？对于一张图片来说，能决定它有没有猫的是每一个像素，因此像素数就是神经元的个数，这是一个很直观的想法。
这个等式的含义就是：我们希望找到一套普适的权重序列，把每张图片的每个像素扔给它们，就能得出图片是否包含猫的判断。这就是神经网络解决问题的基本思路。

怎么找到这批普适的权重序列W呢？如果能够代入m张图，对于有猫的能让结果为1，没猫的结果为0，我们认为这就得到了理想的W，以后再代入从没见过的新图，很可能这个结果P就能反映出是否有猫了。

具体怎么计算这套W呢？这就需要用到梯度下降法了，定义一个非凸的成本函数L来反应在一套固定的权重W下，计算结果和实际结果之间的偏差。先随机取一套W，通过求偏导环顾四周，找到令成本函数下降的方向，让W向这个方向修正，不断循环迭代，直到再也找不到能让L值更小的点，就算找到了最优W。

以上就是我对神经网络的最朴素的理解。接下来还是更规范一些，整理出我对课程理解的笔记。